{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a3c2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql.functions import monotonically_increasing_id, pandas_udf, PandasUDFType, col\n",
    "\n",
    "import IPython.display as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c58c1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:12:25.061012: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 18:12:25.196721: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:25.196741: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-18 18:12:25.232698: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 18:12:25.975396: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:25.975524: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:25.975535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6a2535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyspark :  3.3.1\n",
      "Numpy :  1.21.6\n",
      "Keras :  2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Pyspark : \", pyspark.__version__)\n",
    "print(\"Numpy : \", np.__version__)\n",
    "print(\"Keras : \", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd45f7",
   "metadata": {},
   "source": [
    "# Building pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a251763",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-hadoop-cloud_2.12:3.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94d6851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/anaconda3/envs/project8/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.apache.spark#spark-hadoop-cloud_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-511e07e5-93b7-45a2-ad61-0d6f23fb6f2f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.12;3.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-openstack;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.1 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.14 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.3 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.3 in central\n",
      "\tfound joda-time#joda-time;2.10.10 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.12.3 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.1 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.3.1 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.4.1 in central\n",
      "\tfound org.jdom#jdom;1.1 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound stax#stax-api;1.0.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;3.4.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.0.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-sts;3.0.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ecs;4.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.3.1 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-cos;3.3.1 in central\n",
      "\tfound com.qcloud#cos_api-bundle;5.6.19 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 832ms :: artifacts dl 28ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;3.4.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ecs;4.2.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.0.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-sts;3.0.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.4.1 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.3 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.12.3 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcom.qcloud#cos_api-bundle;5.6.19 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tjoda-time#joda-time;2.10.10 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cos;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-openstack;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.14 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.12;3.2.0 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.jdom#jdom;1.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tstax#stax-api;1.0.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 by [org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   43  |   0   |   0   |   1   ||   42  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-511e07e5-93b7-45a2-ad61-0d6f23fb6f2f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 42 already retrieved (0kB/18ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:12:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-38-66.eu-west-3.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FeatureExtractPreprocess</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f383628d610>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName('FeatureExtractPreprocess') \\\n",
    "    .config('spark.driver.memory', '12g') \\\n",
    "    .config('spark.driver.cores', '4') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222774a0",
   "metadata": {},
   "source": [
    "# Test connection to the S3 bucket and that the data are correclty loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8590f7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/Banana/0_100.jpg\n",
      "Data/Banana/104_100.jpg\n",
      "Data/Banana/107_100.jpg\n",
      "Data/Banana/10_100.jpg\n",
      "Data/Banana/112_100.jpg\n",
      "Data/Banana/117_100.jpg\n",
      "Data/Banana/118_100.jpg\n",
      "Data/Banana/119_100.jpg\n",
      "Data/Banana/11_100.jpg\n",
      "Data/Banana/120_100.jpg\n",
      "Data/Banana/121_100.jpg\n",
      "Data/Banana/122_100.jpg\n",
      "Data/Banana/123_100.jpg\n",
      "Data/Banana/124_100.jpg\n",
      "Data/Banana/125_100.jpg\n",
      "Data/Banana/126_100.jpg\n",
      "Data/Banana/127_100.jpg\n",
      "Data/Banana/128_100.jpg\n",
      "Data/Banana/129_100.jpg\n",
      "Data/Banana/130_100.jpg\n"
     ]
    }
   ],
   "source": [
    "bucket = 'oc8try' # Bucket name\n",
    "subfolder = 'Data'\n",
    "\n",
    "conn = boto3.client('s3')\n",
    "\n",
    "data = conn.list_objects(Bucket=bucket, Prefix=subfolder)['Contents']\n",
    "\n",
    "for i in data[:20]:\n",
    "    print(i['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c12c6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dp.display_jpeg(conn.get_object(Bucket=bucket, Key=data[0]['Key'])['Body'].read(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b972a2",
   "metadata": {},
   "source": [
    "# Load data\n",
    "### Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12b4b2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:12:36 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "root\n",
      " |-- image: struct (nullable = true)\n",
      " |    |-- origin: string (nullable = true)\n",
      " |    |-- height: integer (nullable = true)\n",
      " |    |-- width: integer (nullable = true)\n",
      " |    |-- nChannels: integer (nullable = true)\n",
      " |    |-- mode: integer (nullable = true)\n",
      " |    |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.format('image') \\\n",
    "    .option('pathGlobFilter', '*.jpg') \\\n",
    "    .option('recursiveFileLookup', 'true') \\\n",
    "    .load('s3a://oc8try/Data_subsample')\n",
    "\n",
    "data.persist()\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "054af825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe7b01",
   "metadata": {},
   "source": [
    "### Put raw data into spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd7e6a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "#LABELS\n",
    "data_labels = data.select('image.origin').rdd.map(\n",
    "        lambda x: x.origin.split('/')[-2].replace('.jpg', '')\n",
    "    )\n",
    "data_labels.persist()\n",
    "data_labels = data_labels.map(\n",
    "        lambda x: pyspark.sql.Row(label=x)\n",
    "    ).toDF().withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "#FILENAMES\n",
    "data_filenames = data.select('image.origin').rdd.map(\n",
    "        lambda x: x.origin.split('Data_subsample/')[-1].replace('.jpg', '')\n",
    "    )\n",
    "data_filenames.persist()\n",
    "data_filenames = data_filenames.map(\n",
    "        lambda x: pyspark.sql.Row(filename=x)\n",
    "    ).toDF().withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "#PIXEL VALUES TO MATRIX\n",
    "data_values = data.select('image.data').rdd.map(\n",
    "        lambda x: np.reshape(x.data, [100, 100, 3]).tolist()\n",
    "    )\n",
    "data_values.persist()    \n",
    "data_values = data_values.map(\n",
    "        lambda x: pyspark.sql.Row(values=x)\n",
    "    ).toDF()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a354b29",
   "metadata": {},
   "source": [
    "# Create and setup VGG16 model\n",
    "#### This model allows for a first dimensionality reduction as we start from 100x100x3 values and end up with 4608 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db07131d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:12:44.185404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:44.185439: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-18 18:12:44.185461: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-38-66): /proc/driver/nvidia/version does not exist\n",
      "2022-11-18 18:12:44.185717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 100, 100, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 100, 100, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 50, 50, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 50, 50, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 25, 25, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 25, 25, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 12, 12, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 12, 12, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 6, 6, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten\n",
    "\n",
    "model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(100, 100, 3))\n",
    "model = Model(inputs=model.inputs, outputs=Flatten()(model.output))\n",
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76489581",
   "metadata": {},
   "source": [
    "### Feature extraction from raw data, stored in spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d470637f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://8bbbf480-7dff-4141-ab94-ff66747fab61/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://8bbbf480-7dff-4141-ab94-ff66747fab61/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[image: struct<origin:string,height:int,width:int,nChannels:int,mode:int,data:binary>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def featurize(image):\n",
    "    values_tensor = tf.convert_to_tensor(image.tolist())\n",
    "    return pd.Series(model.predict(values_tensor).tolist())\n",
    "\n",
    "featurize_udf = pandas_udf(featurize, ArrayType(FloatType()))\n",
    "\n",
    "features = data_values.select(featurize_udf(data_values.values).alias('features')).withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "features.persist()\n",
    "data.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d1fcdb",
   "metadata": {},
   "source": [
    "## Split features list to several columns (1 column per feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c928fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 18:12:49.686636: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 18:12:49.714924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 18:12:49.797369: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 18:12:49.816066: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 18:12:49.827938: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:49.828028: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-18 18:12:49.854389: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:49.854557: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-18 18:12:49.864590: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 18:12:49.889744: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 18:12:49.940545: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:49.940671: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-18 18:12:49.949511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:49.949585: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-18 18:12:49.976675: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 18:12:49.984946: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-18 18:12:50.586207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:50.586355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:50.586497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-18 18:12:50.601811: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:50.601964: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:50.602043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-18 18:12:50.688270: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:50.688417: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:50.688460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-18 18:12:50.693334: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:50.693533: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:50.693581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-11-18 18:12:54.712277: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:54.712486: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-18 18:12:54.712594: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-38-66): /proc/driver/nvidia/version does not exist\n",
      "2022-11-18 18:12:54.712908: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 18:12:54.729140: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:54.729315: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-18 18:12:54.729396: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-38-66): /proc/driver/nvidia/version does not exist\n",
      "2022-11-18 18:12:54.729836: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 18:12:54.757499: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:54.757591: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-18 18:12:54.757632: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-38-66): /proc/driver/nvidia/version does not exist\n",
      "2022-11-18 18:12:54.757864: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 18:12:54.786071: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-18 18:12:54.786322: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-18 18:12:54.786444: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-38-66): /proc/driver/nvidia/version does not exist\n",
      "2022-11-18 18:12:54.786778: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 2s 2s/step                   (2 + 2) / 4]\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "nb_features = len(features.select('features').collect()[0][0])\n",
    "features = features.select([F.col('features')[i] for i in range(nb_features)]).withColumn('id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be02811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4609"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd287b9",
   "metadata": {},
   "source": [
    "### Join dataframe with labels, file names and extracted features, export to the S3 bucket as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89bf9803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:13:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/11/18 18:13:25 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:13:32 WARN DAGScheduler: Broadcasting large task binary with size 1966.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:13:35 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join = data_labels.join(data_filenames, on='id')\n",
    "df_join = df_join.join(features, on='id')\n",
    "\n",
    "df_join.write.csv('s3a://oc8try/preprocessing/preprocessed_features.csv', mode='overwrite')\n",
    "\n",
    "#features.unpersist()    # Uncomment if no PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734419d",
   "metadata": {},
   "source": [
    "# Optional: 2nd dimensionality reduction (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ced6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, PCA\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "895f6e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:04 ERROR CodeGenerator: failed to compile: org.codehaus.janino.InternalCompilerException: Compiling \"GeneratedClass\" in \"generated.java\": Code of method \"processNext()V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1\" grows beyond 64 KB\n",
      "org.codehaus.janino.InternalCompilerException: Compiling \"GeneratedClass\" in \"generated.java\": Code of method \"processNext()V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1\" grows beyond 64 KB\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:366)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:336)\n",
      "\tat org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:333)\n",
      "\tat org.codehaus.janino.Java$CompilationUnit.accept(Java.java:363)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:333)\n",
      "\tat org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:235)\n",
      "\tat org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:464)\n",
      "\tat org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:314)\n",
      "\tat org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:237)\n",
      "\tat org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205)\n",
      "\tat org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1490)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1587)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1584)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1437)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:726)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:725)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec.doExecute(ObjectHashAggregateExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:176)\n",
      "\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:260)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:258)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:258)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:230)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:372)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:345)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2870)\n",
      "\tat org.apache.spark.sql.Dataset.first(Dataset.scala:2877)\n",
      "\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:113)\n",
      "\tat org.apache.spark.ml.feature.StandardScaler.fit(StandardScaler.scala:84)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.codehaus.janino.InternalCompilerException: Code of method \"processNext()V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1\" grows beyond 64 KB\n",
      "\tat org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1051)\n",
      "\tat org.codehaus.janino.CodeContext.write(CodeContext.java:932)\n",
      "\tat org.codehaus.janino.UnitCompiler.writeOpcode(UnitCompiler.java:12101)\n",
      "\tat org.codehaus.janino.UnitCompiler.store(UnitCompiler.java:11772)\n",
      "\tat org.codehaus.janino.UnitCompiler.store(UnitCompiler.java:11756)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2597)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1506)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3712)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1848)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1501)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.Java$WhileStatement.accept(Java.java:3245)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3420)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1362)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1335)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:807)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:975)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:392)\n",
      "\tat org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:384)\n",
      "\tat org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1445)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1312)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:833)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:410)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:389)\n",
      "\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:384)\n",
      "\tat org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1594)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:362)\n",
      "\t... 91 more\n",
      "22/11/18 18:14:04 WARN WholeStageCodegenExec: Whole-stage codegen disabled for plan (id=1):\n",
      " *(1) Project [UDF(struct(features[0]_double_VectorAssembler_c1698e3435ba, cast(features#169[0] as double), features[1]_double_VectorAssembler_c1698e3435ba, cast(features#169[1] as double), features[2]_double_VectorAssembler_c1698e3435ba, cast(features#169[2] as double), features[3]_double_VectorAssembler_c1698e3435ba, cast(features#169[3] as double), features[4]_double_VectorAssembler_c1698e3435ba, cast(features#169[4] as double), features[5]_double_VectorAssembler_c1698e3435ba, cast(features#169[5] as double), features[6]_double_VectorAssembler_c1698e3435ba, cast(features#169[6] as double), features[7]_double_VectorAssembler_c1698e3435ba, cast(features#169[7] as double), features[8]_double_VectorAssembler_c1698e3435ba, cast(features#169[8] as double), features[9]_double_VectorAssembler_c1698e3435ba, cast(features#169[9] as double), features[10]_double_VectorAssembler_c1698e3435ba, cast(features#169[10] as double), features[11]_double_VectorAssembler_c1698e3435ba, cast(features#169[11] as double), ... 9192 more fields)) AS vect_features#28047]\n",
      "+- InMemoryTableScan [features#169, id#171L]\n",
      "      +- InMemoryRelation [features#169, id#171L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- *(2) Project [pythonUDF0#174 AS features#169, monotonically_increasing_id() AS id#171L]\n",
      "               +- ArrowEvalPython [featurize(values#166)#168], [pythonUDF0#174], 200\n",
      "                  +- *(1) Scan ExistingRDD[values#166]\n",
      "\n",
      "22/11/18 18:14:05 WARN DAGScheduler: Broadcasting large task binary with size 1419.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:09 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:35 ERROR CodeGenerator: failed to compile: org.codehaus.janino.InternalCompilerException: Compiling \"GeneratedClass\" in \"generated.java\": Code of method \"processNext()V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1\" grows beyond 64 KB\n",
      "org.codehaus.janino.InternalCompilerException: Compiling \"GeneratedClass\" in \"generated.java\": Code of method \"processNext()V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1\" grows beyond 64 KB\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:366)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:336)\n",
      "\tat org.codehaus.janino.UnitCompiler$1.visitCompilationUnit(UnitCompiler.java:333)\n",
      "\tat org.codehaus.janino.Java$CompilationUnit.accept(Java.java:363)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:333)\n",
      "\tat org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:235)\n",
      "\tat org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:464)\n",
      "\tat org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:314)\n",
      "\tat org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:237)\n",
      "\tat org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205)\n",
      "\tat org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1490)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1587)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1584)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
      "\tat org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)\n",
      "\tat org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)\n",
      "\tat org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1437)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:726)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:725)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:173)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:172)\n",
      "\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3396)\n",
      "\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:3394)\n",
      "\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:89)\n",
      "\tat org.apache.spark.ml.feature.PCA.fit(PCA.scala:64)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.codehaus.janino.InternalCompilerException: Code of method \"processNext()V\" of class \"org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1\" grows beyond 64 KB\n",
      "\tat org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:1051)\n",
      "\tat org.codehaus.janino.CodeContext.write(CodeContext.java:932)\n",
      "\tat org.codehaus.janino.UnitCompiler.writeOpcode(UnitCompiler.java:12101)\n",
      "\tat org.codehaus.janino.UnitCompiler.store(UnitCompiler.java:11772)\n",
      "\tat org.codehaus.janino.UnitCompiler.store(UnitCompiler.java:11756)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2597)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1506)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3712)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1559)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1496)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.Java$Block.accept(Java.java:2969)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1848)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$2200(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1501)\n",
      "\tat org.codehaus.janino.UnitCompiler$6.visitWhileStatement(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.Java$WhileStatement.accept(Java.java:3245)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1490)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1573)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3420)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1362)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1335)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:807)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:975)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:392)\n",
      "\tat org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:384)\n",
      "\tat org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1445)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\n",
      "\tat org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1312)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:833)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:410)\n",
      "\tat org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:226)\n",
      "\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:389)\n",
      "\tat org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:384)\n",
      "\tat org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1594)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:384)\n",
      "\tat org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:362)\n",
      "\t... 53 more\n",
      "22/11/18 18:14:35 WARN WholeStageCodegenExec: Whole-stage codegen disabled for plan (id=1):\n",
      " *(1) Project [UDF(vect_features#28047) AS scaled_features#32745]\n",
      "+- *(1) Project [UDF(struct(features[0]_double_VectorAssembler_c1698e3435ba, cast(features#169[0] as double), features[1]_double_VectorAssembler_c1698e3435ba, cast(features#169[1] as double), features[2]_double_VectorAssembler_c1698e3435ba, cast(features#169[2] as double), features[3]_double_VectorAssembler_c1698e3435ba, cast(features#169[3] as double), features[4]_double_VectorAssembler_c1698e3435ba, cast(features#169[4] as double), features[5]_double_VectorAssembler_c1698e3435ba, cast(features#169[5] as double), features[6]_double_VectorAssembler_c1698e3435ba, cast(features#169[6] as double), features[7]_double_VectorAssembler_c1698e3435ba, cast(features#169[7] as double), features[8]_double_VectorAssembler_c1698e3435ba, cast(features#169[8] as double), features[9]_double_VectorAssembler_c1698e3435ba, cast(features#169[9] as double), features[10]_double_VectorAssembler_c1698e3435ba, cast(features#169[10] as double), features[11]_double_VectorAssembler_c1698e3435ba, cast(features#169[11] as double), ... 9192 more fields)) AS vect_features#28047]\n",
      "   +- InMemoryTableScan [features#169, id#171L]\n",
      "         +- InMemoryRelation [features#169, id#171L], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               +- *(2) Project [pythonUDF0#174 AS features#169, monotonically_increasing_id() AS id#171L]\n",
      "                  +- ArrowEvalPython [featurize(values#166)#168], [pythonUDF0#174], 200\n",
      "                     +- *(1) Scan ExistingRDD[values#166]\n",
      "\n",
      "22/11/18 18:14:36 WARN DAGScheduler: Broadcasting large task binary with size 1455.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:37 WARN DAGScheduler: Broadcasting large task binary with size 1455.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:38 WARN DAGScheduler: Broadcasting large task binary with size 1457.1 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:40 WARN DAGScheduler: Broadcasting large task binary with size 1455.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:41 WARN DAGScheduler: Broadcasting large task binary with size 1455.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:43 WARN DAGScheduler: Broadcasting large task binary with size 1456.2 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:14:47 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/11/18 18:14:47 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "22/11/18 18:18:57 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|        pca_features|\n",
      "+---+--------------------+\n",
      "|  0|[-25.062279051126...|\n",
      "|  1|[-23.464313308914...|\n",
      "|  2|[-23.779500312017...|\n",
      "|  3|[-23.136970895944...|\n",
      "|  4|[-22.760990110762...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "list_feat = [col for col in features.columns if 'features' in col]\n",
    "    \n",
    "assembler = VectorAssembler(inputCols=list_feat, outputCol='vect_features') # assemble feature columns to vector\n",
    "scaler = StandardScaler(inputCol='vect_features', outputCol='scaled_features') # scale feature vector\n",
    "pca = PCA(k=2, inputCol='scaled_features', outputCol='pca_features') # pca on scaled features\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, scaler, pca])\n",
    "\n",
    "features = pipeline.fit(features).transform(features).drop(*list_feat).drop(*['vect_features', 'scaled_features'])\n",
    "\n",
    "features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6266c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:19:00 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|        pca_features|\n",
      "+---+--------------------+\n",
      "|  0|[-25.062279051126...|\n",
      "|  1|[-23.464313308914...|\n",
      "|  2|[-23.779500312017...|\n",
      "|  3|[-23.136970895944...|\n",
      "|  4|[-22.760990110762...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b297443",
   "metadata": {},
   "source": [
    "## Export to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdc90bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/18 18:19:20 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join_pca = data_labels.join(data_filenames, on='id')\n",
    "df_join_pca = df_join_pca.join(features, on='id')\n",
    "\n",
    "df_join_pca.write.parquet('s3a://oc8try/preprocessing/preprocessed_features_pca.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6f95ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pd_pca = df_join_pca.toPandas()\n",
    "#df_pd_pca\n",
    "#df_pd_pca.to_csv('s3a://oc8try/preprocessing/preprocessed_features_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d311ba9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: string, id: bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.unpersist()\n",
    "data_values.unpersist()\n",
    "data_filenames.unpersist()\n",
    "data_labels.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
